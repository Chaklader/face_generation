{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affaafaf",
   "metadata": {},
   "source": [
    "# Face Generation\n",
    "\n",
    "In this project, you'll define and train a Generative Adverserial network of your own creation on a dataset of faces. Your goal is to get a generator network to generate *new* images of faces that look as realistic as possible!\n",
    "\n",
    "The project will be broken down into a series of tasks from **defining new architectures training adversarial networks**. At the end of the notebook, you'll be able to visualize the results of your trained Generator to see how it performs; your generated samples should look like fairly realistic faces with small amounts of noise.\n",
    "\n",
    "### Get the Data\n",
    "\n",
    "You'll be using the [CelebFaces Attributes Dataset (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) to train your adversarial networks.\n",
    "\n",
    "This dataset has higher resolution images than datasets you have previously worked with (like MNIST or SVHN) you've been working with, and so, you should prepare to define deeper networks and train them for a longer time to get good results. It is suggested that you utilize a GPU for training.\n",
    "\n",
    "### Pre-processed Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. Some sample data is show below.\n",
    "\n",
    "<img src='assets/processed_face_data.png' width=60% />\n",
    "\n",
    "> If you are working locally, you can download this data [by clicking here](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be7eb6f_processed-celeba-small/processed-celeba-small.zip)\n",
    "\n",
    "This is a zip file that you'll need to extract in the home directory of this notebook for further loading and processing. After extracting the data, you should be left with a directory of data `processed-celeba-small/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55313811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T03:36:34.021940Z",
     "start_time": "2024-11-01T03:36:21.944712Z"
    }
   },
   "outputs": [],
   "source": [
    "# run this once to unzip the file\n",
    "!unzip processed-celeba-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9f97f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T01:50:34.760233Z",
     "start_time": "2024-11-02T01:50:32.128805Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    "    Resize\n",
    ")\n",
    "\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e793a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T01:50:36.238458Z",
     "start_time": "2024-11-02T01:50:36.236458Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'processed_celeba_small/celeba/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b320a1",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "The [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset contains over 200,000 celebrity images with annotations. Since you're going to be generating faces, you won't need the annotations, you'll only need the images. Note that these are color images with [3 color channels (RGB)](https://en.wikipedia.org/wiki/Channel_(digital_image)#RGB_Images) each.\n",
    "\n",
    "### Pre-process and Load the Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. This *pre-processed* dataset is a smaller subset of the very large CelebA dataset and contains roughly 30,000 images. \n",
    "\n",
    "Your first task consists in building the dataloader. To do so, you need to do the following:\n",
    "* implement the get_transforms function\n",
    "* create a custom Dataset class that reads the CelebA data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc09433",
   "metadata": {},
   "source": [
    "### Exercise: implement the get_transforms function\n",
    "\n",
    "The `get_transforms` function should output a [`torchvision.transforms.Compose`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose) of different transformations. You have two constraints:\n",
    "* the function takes a tuple of size as input and should **resize the images** to the input size\n",
    "* the output images should have values **ranging from -1 to 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284fc64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:16.181724Z",
     "start_time": "2024-11-01T12:55:16.178508Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_transforms(size: Tuple[int, int]) -> Callable:\n",
    "    \"\"\"\n",
    "    Transforms to apply to the image.\n",
    "    Args:\n",
    "        size: Target size of the images\n",
    "    Returns:\n",
    "        Composition of transforms\n",
    "    \"\"\"\n",
    "    transforms = [\n",
    "        Resize(size),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ]\n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426529d",
   "metadata": {},
   "source": [
    "### Exercise: implement the DatasetDirectory class\n",
    "\n",
    "\n",
    "The `DatasetDirectory` class is a torch Dataset that reads from the above data directory. The `__getitem__` method should output a transformed tensor and the `__len__` method should output the number of files in our dataset. You can look at [this custom dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) for ideas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06bde9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:20.981970Z",
     "start_time": "2024-11-01T12:55:20.977940Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class DatasetDirectory(Dataset):\n",
    "    \"\"\"A PyTorch Dataset class for loading images from a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to directory containing image files\n",
    "        transforms (Callable, optional): Transformations to apply to images. Defaults to basic 64x64 transforms.\n",
    "        extension (str, optional): File extension to filter by. Defaults to '.jpg'.\n",
    "    \n",
    "    Attributes:\n",
    "        directory (str): Directory containing images\n",
    "        image_files (list): Sorted list of image file paths\n",
    "        transforms (Callable): Image transformation function\n",
    "    \n",
    "    Methods:\n",
    "        __len__(): Returns number of images in dataset\n",
    "        __getitem__(index): Returns transformed image tensor at given index\n",
    "    \n",
    "    Example:\n",
    "        >>> dataset = DatasetDirectory('path/to/images')\n",
    "        >>> len(dataset)  # Number of images\n",
    "        100\n",
    "        >>> image = dataset[0]  # First image tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, directory: str, transforms: Callable = None, extension: str = '.jpg'):\n",
    "        \"\"\"Initialize dataset with directory of images.\"\"\"\n",
    "        self.directory = directory\n",
    "        self.image_files = sorted([\n",
    "            os.path.join(directory, f)\n",
    "            for f in os.listdir(directory)\n",
    "            if f.endswith(extension)\n",
    "        ])\n",
    "\n",
    "        # Default transform if none provided\n",
    "        self.transforms = transforms if transforms else get_transforms((64, 64))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \"\"\"Get a single image.\"\"\"\n",
    "        # Get image path\n",
    "        image_path = self.image_files[index]\n",
    "\n",
    "        # Load and convert image to RGB\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd86e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:23.925874Z",
     "start_time": "2024-11-01T12:55:23.810386Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "dataset = DatasetDirectory(data_dir, get_transforms((64, 64)))\n",
    "tests.check_dataset_outputs(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd03e30",
   "metadata": {},
   "source": [
    "The functions below will help you visualize images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629657a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:32.920071Z",
     "start_time": "2024-11-01T12:55:32.422961Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def denormalize(images):\n",
    "    \"\"\"Transform images from [-1.0, 1.0] to [0, 255] and cast them to uint8.\n",
    "        Convert normalized image tensors back to displayable uint8 format.\n",
    "    \n",
    "    Args:\n",
    "        images (numpy.ndarray): Image array in range [-1.0, 1.0]\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Image array in range [0, 255] as uint8\n",
    "    \n",
    "    Example:\n",
    "        >>> normalized = np.random.uniform(-1, 1, (3, 64, 64))\n",
    "        >>> display_ready = denormalize(normalized)  # (64, 64, 3) uint8 array\n",
    "    \"\"\"\n",
    "    return ((images + 1.) / 2. * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Visualization code documentation:\n",
    "\"\"\"\n",
    "Visualizes a batch of images from the dataset.\n",
    "\n",
    "Creates a 2-row grid of images with:\n",
    "- 20 total subplots (10 per row) \n",
    "- No axis ticks\n",
    "- Each image is:\n",
    "  1. Converted to numpy array\n",
    "  2. Transposed from (C,H,W) to (H,W,C) \n",
    "  3. Denormalized from [-1,1] to [0,255]\n",
    "  4. Displayed using matplotlib\n",
    "\n",
    "# Common PyTorch image tensor dimensions:\n",
    "W = Width    # Image width in pixels\n",
    "H = Height   # Image height in pixels  \n",
    "C = Channel  # Color channels (3 for RGB, 1 for grayscale)\n",
    "\n",
    "# Typical shape orders:\n",
    "# PyTorch: (C, H, W) - Channels first\n",
    "# NumPy/matplotlib: (H, W, C) - Channels last\n",
    "\n",
    "Parameters:\n",
    "- figsize: Tuple controlling figure dimensions (20,4) inches\n",
    "- plot_size: Number of images to display (20)\n",
    "- dataset: PyTorch dataset containing normalized images\n",
    "\n",
    "plot the images in the batch, along with the corresponding labels\n",
    "\"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(5, 1))\n",
    "plot_size = 5\n",
    "\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(1, plot_size, idx + 1, xticks=[], yticks=[])\n",
    "    img = dataset[idx].numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = denormalize(img)\n",
    "    \n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef21cc",
   "metadata": {},
   "source": [
    "## Model implementation\n",
    "\n",
    "As you know, a GAN is comprised of two adversarial networks, a discriminator and a generator. Now that we have a working data pipeline, we need to implement the discriminator and the generator. \n",
    "\n",
    "Feel free to implement any additional class or function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60564704",
   "metadata": {},
   "source": [
    "### Exercise: Create the discriminator\n",
    "\n",
    "The discriminator's job is to score real and fake images. You have two constraints here:\n",
    "* the discriminator takes as input a **batch of 64x64x3 images**\n",
    "* the output should be a single value (=score)\n",
    "\n",
    "Feel free to get inspiration from the different architectures we talked about in the course, such as DCGAN, WGAN-GP or DRAGAN.\n",
    "\n",
    "#### Some tips\n",
    "* To scale down from the input image, you can either use `Conv2d` layers with the correct hyperparameters or Pooling layers.\n",
    "* If you plan on using gradient penalty, do not use Batch Normalization layers in the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80581099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:36.808690Z",
     "start_time": "2024-11-01T12:55:36.806849Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f3e3b",
   "metadata": {},
   "source": [
    "#### Generative Adversarial Networks: The Discriminator\n",
    "\n",
    "##### The Discriminator Component in GANs\n",
    "\n",
    "The discriminator in a Generative Adversarial Network (GAN) functions as a binary classifier that distinguishes between real data samples and synthetic samples produced by the generator. This critical component provides the learning signal that drives the generator's improvement through adversarial training.\n",
    "\n",
    "##### Theoretical Foundation of the Discriminator\n",
    "\n",
    "In the GAN framework, introduced by Goodfellow et al. (2014), the discriminator (D) and generator (G) engage in a minimax game defined by the following objective function:\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "Where:\n",
    "- $D(x)$ represents the discriminator's estimate of the probability that real data $x$ is real\n",
    "- $G(z)$ is the generator's output when given noise $z$\n",
    "- $p_{data}$ is the real data distribution\n",
    "- $p_z$ is the noise distribution (typically uniform or Gaussian)\n",
    "\n",
    "The discriminator aims to maximize this function, which means:\n",
    "- Maximizing $\\log D(x)$ for real samples (correctly identifying real as real)\n",
    "- Maximizing $\\log(1-D(G(z)))$ for fake samples (correctly identifying fake as fake)\n",
    "\n",
    "##### DCGAN Discriminator Architecture\n",
    "\n",
    "The provided code implements a Deep Convolutional GAN (DCGAN) discriminator, following design principles from Radford et al. (2015). Let's analyze the actual implementation:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Input[\"Input: RGB Image<br>3×64×64\"] --> Conv1[\"Conv1: 3→64 channels<br>4×4 kernel, stride=2<br>64×32×32\"]\n",
    "    Conv1 --> LeakyReLU1[\"LeakyReLU<br>α=0.2\"]\n",
    "    LeakyReLU1 --> Conv2[\"Conv2: 64→128 channels<br>4×4 kernel, stride=2<br>128×16×16\"]\n",
    "    Conv2 --> BatchNorm2[\"Batch Normalization\"]\n",
    "    BatchNorm2 --> LeakyReLU2[\"LeakyReLU<br>α=0.2\"]\n",
    "    LeakyReLU2 --> Conv3[\"Conv3: 128→256 channels<br>4×4 kernel, stride=2<br>256×8×8\"]\n",
    "    Conv3 --> BatchNorm3[\"Batch Normalization\"]\n",
    "    BatchNorm3 --> LeakyReLU3[\"LeakyReLU<br>α=0.2\"]\n",
    "    LeakyReLU3 --> Conv4[\"Conv4: 256→512 channels<br>4×4 kernel, stride=2<br>512×4×4\"]\n",
    "    Conv4 --> BatchNorm4[\"Batch Normalization\"]\n",
    "    BatchNorm4 --> LeakyReLU4[\"LeakyReLU<br>α=0.2\"]\n",
    "    LeakyReLU4 --> Conv5[\"Conv5: 512→1 channel<br>4×4 kernel, stride=1<br>1×1×1\"]\n",
    "    Conv5 --> Sigmoid[\"Sigmoid Activation\"]\n",
    "    Sigmoid --> Output[\"Output: Probability<br>Real vs. Fake\"]\n",
    "    \n",
    "    style Input fill:#BCFB89\n",
    "    style Conv1 fill:#9AE4F5\n",
    "    style Conv2 fill:#9AE4F5\n",
    "    style Conv3 fill:#9AE4F5\n",
    "    style Conv4 fill:#9AE4F5\n",
    "    style Conv5 fill:#9AE4F5\n",
    "    style LeakyReLU1 fill:#FBF266\n",
    "    style LeakyReLU2 fill:#FBF266\n",
    "    style LeakyReLU3 fill:#FBF266\n",
    "    style LeakyReLU4 fill:#FBF266\n",
    "    style BatchNorm2 fill:#FA756A\n",
    "    style BatchNorm3 fill:#FA756A\n",
    "    style BatchNorm4 fill:#FA756A\n",
    "    style Sigmoid fill:#0096D9\n",
    "    style Output fill:#FCEB14\n",
    "```\n",
    "\n",
    "##### Detailed Architecture Analysis\n",
    "\n",
    "Examining the provided implementation reveals several key architectural decisions:\n",
    "\n",
    "1. **Convolutional Layers with Precise Parameterization**:\n",
    "   - All convolutional layers use 4×4 kernels, which is a standard choice in DCGAN\n",
    "   - First layer: $3 \\rightarrow 64$ channels (RGB input to first feature set)\n",
    "   - Progressive channel doubling: $64 \\rightarrow 128 \\rightarrow 256 \\rightarrow 512$\n",
    "   - Final layer: $512 \\rightarrow 1$ (single output for binary classification)\n",
    "\n",
    "2. **Downsampling Strategy**:\n",
    "   - Stride=2 for spatial downsampling in the first four layers (halving dimensions each time)\n",
    "   - Stride=1 in the final layer to reach 1×1 spatial dimensions\n",
    "\n",
    "   The spatial dimension progression follows:\n",
    "   $$64 \\times 64 \\rightarrow 32 \\times 32 \\rightarrow 16 \\times 16 \\rightarrow 8 \\times 8 \\rightarrow 4 \\times 4 \\rightarrow 1 \\times 1$$\n",
    "\n",
    "3. **Activation Functions**:\n",
    "   - LeakyReLU with negative slope 0.2 for all intermediate layers\n",
    "   - Sigmoid activation for the final layer, constraining output to [0,1]\n",
    "\n",
    "4. **Normalization**:\n",
    "   - BatchNorm2d applied after each convolutional layer except the first and last\n",
    "   - First layer omits BatchNorm to allow the model to learn from the input distribution\n",
    "   - Final layer omits BatchNorm to avoid constraining the output distribution\n",
    "\n",
    "5. **Bias Parameter**:\n",
    "   - All convolutional layers set `bias=False` when batch normalization follows\n",
    "   - This reduces redundant parameters as the BatchNorm already includes a learnable bias\n",
    "\n",
    "##### Implementation Details\n",
    "\n",
    "The code reveals several important implementation techniques:\n",
    "\n",
    "```python\n",
    "class Discriminator(Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # DCGAN-style discriminator\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: 3x64x64\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 64x32x32\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 128x16x16\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 256x8x8\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 512x4x4 -> 1x1x1x1\n",
    "            nn.Conv2d(512, 1, 4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "```\n",
    "\n",
    "Notable aspects include:\n",
    "\n",
    "1. **Sequential Container**: Using `nn.Sequential` for clean, compact layer organization\n",
    "\n",
    "2. **Padding Strategy**:\n",
    "   - `padding=1` in the first four layers maintains appropriate spatial dimensions\n",
    "   - `padding=0` in the final layer ensures reduction to 1×1 output\n",
    "\n",
    "3. **inplace=True** for LeakyReLU:\n",
    "   - Modifies input directly rather than creating a new tensor\n",
    "   - Improves memory efficiency during training\n",
    "\n",
    "##### The Forward Pass\n",
    "\n",
    "The `forward` method processes input through the network:\n",
    "\n",
    "```python\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Forward pass for discriminator.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor of shape (N, C, H, W)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Probability scores of shape (N, 1)\n",
    "    \"\"\"\n",
    "    return self.main(x).view(-1, 1, 1, 1)\n",
    "```\n",
    "\n",
    "This method:\n",
    "1. Passes the input tensor through the sequential layers\n",
    "2. Reshapes the output using `.view(-1, 1, 1, 1)` to maintain a 4D tensor structure\n",
    "   - This is an interesting choice as most implementations would use `.view(-1, 1)` for a batch of scalar outputs\n",
    "   - The additional dimensions might be used for compatibility with specific loss functions or architectures\n",
    "\n",
    "##### Mathematical Analysis of the Discriminator's Operation\n",
    "\n",
    "For an input image $x \\in \\mathbb{R}^{3 \\times 64 \\times 64}$, the forward pass can be expressed as:\n",
    "\n",
    "1. First layer: $h_1 = \\text{LeakyReLU}(\\text{Conv2D}(x))$\n",
    "2. Intermediate layers: $h_{i+1} = \\text{LeakyReLU}(\\text{BatchNorm}(\\text{Conv2D}(h_i)))$ for $i \\in \\{1, 2, 3\\}$\n",
    "3. Final layer: $y = \\sigma(\\text{Conv2D}(h_4))$\n",
    "\n",
    "Where $\\sigma$ is the sigmoid function:\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "##### Training Dynamics and Optimization\n",
    "\n",
    "During training, the discriminator is optimized to maximize:\n",
    "\n",
    "$$\\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "In practice, this translates to minimizing the binary cross-entropy loss:\n",
    "\n",
    "$$\\mathcal{L}_D = -\\frac{1}{m} \\sum_{i=1}^{m} [\\log D(x^{(i)}) + \\log(1 - D(G(z^{(i)})))]$$\n",
    "\n",
    "Where:\n",
    "- $m$ is the batch size\n",
    "- $x^{(i)}$ are real data samples\n",
    "- $z^{(i)}$ are noise samples\n",
    "\n",
    "##### Advanced Training Techniques\n",
    "\n",
    "While the architecture above represents the standard DCGAN discriminator, several techniques can enhance training stability:\n",
    "\n",
    "1. **Label Smoothing**: Using targets of 0.9 instead of 1.0 for real samples:\n",
    "   ```python\n",
    "   real_labels = torch.full((batch_size,), 0.9, device=device)\n",
    "   ```\n",
    "\n",
    "2. **Noise Injection**: Adding small Gaussian noise to inputs:\n",
    "   ```python\n",
    "   real_images += 0.05 * torch.randn_like(real_images)\n",
    "   ```\n",
    "\n",
    "3. **One-sided Label Smoothing**: Only smoothing the real labels (not fake ones)\n",
    "\n",
    "4. **Feature Matching**: Using intermediate features for additional loss terms\n",
    "\n",
    "##### Spectral Normalization Enhancement\n",
    "\n",
    "A common enhancement to the discriminator is spectral normalization, which constrains the Lipschitz constant of each layer:\n",
    "\n",
    "```python\n",
    "# Modified layer with spectral normalization\n",
    "nn.utils.spectral_norm(nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False)),\n",
    "```\n",
    "\n",
    "This technique stabilizes training by preventing extreme gradients and mode collapse.\n",
    "\n",
    "##### Conclusion\n",
    "\n",
    "The DCGAN discriminator architecture presented in the code exemplifies a well-structured convolutional network designed for binary classification of images. It incorporates key principles that have made DCGANs successful:\n",
    "\n",
    "1. Progressive downsampling using strided convolutions\n",
    "2. Gradually increasing feature channels\n",
    "3. LeakyReLU activations for better gradient flow\n",
    "4. Batch normalization for training stability\n",
    "5. Full convolutional architecture without fully connected layers\n",
    "\n",
    "This architecture has served as a foundation for numerous GAN variants and continues to be relevant in modern generative modeling. The discriminator's ability to learn a complex decision boundary in high-dimensional space is what enables the generator to produce increasingly realistic outputs through adversarial training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5832c200",
   "metadata": {},
   "source": [
    "---\n",
    "#### DCGAN-style discriminator network for image classification.\n",
    "\n",
    "##### Architecture Overview\n",
    "\n",
    "- Input: (N, 3, 64, 64) tensor (batch of 64x64 RGB images)\n",
    "- Output: (N, 1) tensor (probability scores ∈ [0,1])\n",
    "- Key Components:\n",
    "- 4 convolutional layers with LeakyReLU(0.2)\n",
    "- Batch normalization (except input layer)\n",
    "- Strided convolutions (downsampling)\n",
    "- Sigmoid activation for binary classification\n",
    "\n",
    "The notation 3x64x64 refers to the input tensor dimensions, while the numbers in Conv2d(3, 64, 4,...) are the layer parameters. Here's the breakdown:\n",
    "\n",
    "Input Dimensions (3x64x64):\n",
    "- 3: Channels (RGB)\n",
    "- 64: Height (pixels)\n",
    "- 64: Width (pixels)\n",
    "\n",
    "Conv2d Parameters (3, 64, 4):\n",
    "- in_channels=3: Matches the input's 3 RGB channels.\n",
    "- out_channels=64: Creates 64 output feature maps. So we need to use 64 filters/ kernels for 64 output feature maps.\n",
    "- kernel_size=4: Uses a 4x4 convolutional filter.\n",
    "\n",
    "How Dimensions Transform:\n",
    "- Input: (batch_size, 3, 64, 64)\n",
    "- After Conv2d:\n",
    "  - Spatial size: (64 - 4 + 2*1)/2 + 1 = 32 (due to stride=2, padding=1)\n",
    "  - Output: (batch_size, 64, 32, 32)\n",
    "\n",
    "Key Formula:\n",
    "\n",
    "output_size = (input_size - kernel_size + 2*padding)/stride + 1\n",
    "= (H - F + 2P)/S + 1\n",
    "\n",
    "##### Layer-by-Layer Breakdown\n",
    "\n",
    "1. Input Layer\n",
    "   - Shape: `3x64x64` (RGB image)\n",
    "   - Conv2d: `3 → 64` channels, kernel=4x4, stride=2, padding=1\n",
    "   - Effect: Halves spatial dimensions → `64x32x32`\n",
    "   - LeakyReLU: Negative slope=0.2 (preserves gradient flow)\n",
    "\n",
    "2. Hidden Layer 1\n",
    "   - Shape: `64x32x32`\n",
    "   - Conv2d: `64 → 128` channels, kernel=4x4, stride=2, padding=1\n",
    "   - BatchNorm2d: Normalizes 128-channel output\n",
    "   - LeakyReLU: Same as above → `128x16x16`\n",
    "\n",
    "3. Hidden Layer 2\n",
    "   - Shape: `128x16x16`\n",
    "   - Conv2d: `128 → 256` channels, kernel=4x4, stride=2, padding=1\n",
    "   - BatchNorm2d: Normalizes 256-channel output\n",
    "   - LeakyReLU → `256x8x8`\n",
    "\n",
    "4. Hidden Layer 3\n",
    "   - Shape: `256x8x8`\n",
    "   - Conv2d: `256 → 512` channels, kernel=4x4, stride=2, padding=1\n",
    "   - BatchNorm2d: Normalizes 512-channel output\n",
    "   - LeakyReLU → `512x4x4`\n",
    "\n",
    "5. Output Layer\n",
    "   - Conv2d: `512 → 1` channel, kernel=4x4, stride=1, padding=0\n",
    "   - Sigmoid: Maps to probability ∈ [0,1] → `1x1x1`\n",
    "\n",
    "##### Dimensionality Progression\n",
    "\n",
    "```python\n",
    "3x64x64 → 64x32x32 → 128x16x16 → 256x8x8 → 512x4x4 → 1x1x1   \n",
    "```\n",
    "\n",
    "##### Usage Example\n",
    "\n",
    "```python\n",
    "D = Discriminator()\n",
    "images = torch.randn(32, 3, 64, 64)  # Batch of 32 fake images\n",
    "outputs = D(images)  # Shape: (32, 1)\n",
    "``` \n",
    "\n",
    "##### Why This Architecture Works\n",
    "- Strided Convs: Efficient downsampling without pooling\n",
    "- BatchNorm: Stabilizes training in deep networks\n",
    "- LeakyReLU: Prevents \"dead neurons\" in discriminator\n",
    "- No Bias: BatchNorm makes bias redundant\n",
    "- Sigmoid: Classic choice for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e205c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:38.099895Z",
     "start_time": "2024-11-01T12:55:38.096151Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(Module):\n",
    "    \"\"\"DCGAN-style discriminator network for image classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - 4 convolutional layers with LeakyReLU activation\n",
    "    - Batch normalization in intermediate layers  \n",
    "    - Strided convolutions for downsampling\n",
    "    - Final sigmoid activation for binary classification\n",
    "    \n",
    "    Input: (N, 3, 64, 64) tensor (batch of 64x64 RGB images)\n",
    "    Output: (N, 1) tensor (probability of being real)\n",
    "    \n",
    "    Dimensionality progression:\n",
    "    3x64x64 → 64x32x32 → 128x16x16 → 256x8x8 → 512x4x4 → 1x1x1\n",
    "    \n",
    "    Args:\n",
    "        None (fixed architecture)\n",
    "    \n",
    "    Methods:\n",
    "        forward(x): Processes input tensor through the network\n",
    "    \n",
    "    Example:\n",
    "        >>> D = Discriminator()\n",
    "        >>> images = torch.randn(32, 3, 64, 64)  # Batch of 32 fake images\n",
    "        >>> outputs = D(images)  # Shape: (32, 1)\n",
    "\n",
    "        ##### Shape Transformation Diagram\n",
    "\n",
    "        Input Image:    [N, 3, 64, 64]  # Batch × RGB × Height × Width\n",
    "        │\n",
    "        ▼\n",
    "        Layer 1 Conv:   [N, 64, 32, 32]  # (64-4+2)/2 + 1 = 32\n",
    "        │\n",
    "        ▼\n",
    "        Layer 2 Conv:   [N, 128, 16, 16]  # (32-4+2)/2 + 1 = 16\n",
    "        │\n",
    "        ▼  \n",
    "        Layer 3 Conv:   [N, 256, 8, 8]    # (16-4+2)/2 + 1 = 8\n",
    "        │\n",
    "        ▼\n",
    "        Layer 4 Conv:   [N, 512, 4, 4]    # (8-4+2)/2 + 1 = 4\n",
    "        │\n",
    "        ▼\n",
    "        Output Conv:    [N, 1, 1, 1]      # (4-4+0)/1 + 1 = 1\n",
    "\n",
    "\n",
    "        [3,64,64]              [64,32,32]             [128,16,16]\n",
    "        ┌───────┐              ┌────────┐             ┌─────────┐\n",
    "        │ RGB   │              │ 64 fm  │             │ 128 fm  │\n",
    "        │ Input │──Conv2d─────▶│ 32×32  │──Conv2d────▶│ 16×16   │\n",
    "        └───────┘              └────────┘             └─────────┘\n",
    "                                        \n",
    "        [256,8,8]              [512,4,4]               [1,1,1]\n",
    "        ┌────────┐             ┌───────┐               ┌─────┐\n",
    "        │ 256 fm │             │512 fm │               │Score│\n",
    "        │  8×8   │──Conv2d────▶│ 4×4   │──Conv2d──────▶│ 1×1 │\n",
    "        └────────┘             └───────┘               └─────┘\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes discriminator layers with DCGAN architecture.\"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # DCGAN-style discriminator\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: 3x64x64\n",
    "            nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 64x32x32\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 128x16x16\n",
    "            nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 256x8x8\n",
    "            nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 512x4x4 -> 1x1x1x1\n",
    "            nn.Conv2d(512, 1, 4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for discriminator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (N, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Probability scores of shape (N, 1)\n",
    "        \"\"\"\n",
    "        return self.main(x).view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ec746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:40.900686Z",
     "start_time": "2024-11-01T12:55:40.887343Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "discriminator = Discriminator()\n",
    "tests.check_discriminator(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d54bab1",
   "metadata": {},
   "source": [
    "### Exercise: create the generator\n",
    "\n",
    "The generator's job creates the \"fake images\" and learns the dataset distribution. You have three constraints here:\n",
    "* the generator takes as input a vector of dimension `[batch_size, latent_dimension, 1, 1]`\n",
    "* the generator must outputs **64x64x3 images**\n",
    "\n",
    "Feel free to get inspiration from the different architectures we talked about in the course, such as DCGAN, WGAN-GP or DRAGAN.\n",
    "\n",
    "#### Some tips:\n",
    "* to scale up from the latent vector input, you can use `ConvTranspose2d` layers\n",
    "* as often with Gan, **Batch Normalization** helps with training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee8a5b8",
   "metadata": {},
   "source": [
    "---\n",
    "#### DCGAN-style Generator Network\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "- **Input**: `(N, latent_dim, 1, 1)` tensor (batch of latent vectors)\n",
    "- **Output**: `(N, 3, 64, 64)` tensor (batch of 64x64 RGB images)\n",
    "- **Key Components**:\n",
    "  - 5 transposed convolutional layers (deconvolution)\n",
    "  - Batch normalization after each layer (except output)\n",
    "  - ReLU activations for hidden layers\n",
    "  - Tanh activation for output normalization\n",
    "\n",
    "The generator transforms low-dimensional latent vectors into high-dimensional image space through a series of upsampling operations using transposed convolutions.\n",
    "\n",
    "#### Layer-by-Layer Breakdown\n",
    "\n",
    "1. **Input Layer**  \n",
    "   - Shape: `latent_dim×1×1` (latent vector)  \n",
    "   - ConvTranspose2d: `latent_dim → 512` channels, kernel=4×4, stride=1, padding=0  \n",
    "   - BatchNorm2d: Normalizes 512-channel output  \n",
    "   - ReLU: Positive activation → `512×4×4`\n",
    "\n",
    "2. **Hidden Layer 1**  \n",
    "   - Shape: `512×4×4`  \n",
    "   - ConvTranspose2d: `512 → 256` channels, kernel=4×4, stride=2, padding=1  \n",
    "   - BatchNorm2d: Normalizes 256-channel output  \n",
    "   - ReLU: Positive activation → `256×8×8`\n",
    "\n",
    "3. **Hidden Layer 2**  \n",
    "   - Shape: `256×8×8`  \n",
    "   - ConvTranspose2d: `256 → 128` channels, kernel=4×4, stride=2, padding=1  \n",
    "   - BatchNorm2d: Normalizes 128-channel output  \n",
    "   - ReLU: Positive activation → `128×16×16`\n",
    "\n",
    "4. **Hidden Layer 3**  \n",
    "   - Shape: `128×16×16`  \n",
    "   - ConvTranspose2d: `128 → 64` channels, kernel=4×4, stride=2, padding=1  \n",
    "   - BatchNorm2d: Normalizes 64-channel output  \n",
    "   - ReLU: Positive activation → `64×32×32`\n",
    "\n",
    "5. **Output Layer**  \n",
    "   - Shape: `64×32×32`  \n",
    "   - ConvTranspose2d: `64 → 3` channels, kernel=4×4, stride=2, padding=1  \n",
    "   - Tanh: Maps values to [-1,1] range → `3×64×64`\n",
    "\n",
    "#### Dimensionality Progression\n",
    "\n",
    "latent_dim×1×1 → 512×4×4 → 256×8×8 → 128×16×16 → 64×32×32 → 3×64×64\n",
    "\n",
    "#### Transposed Convolution Mechanics\n",
    "\n",
    "The output dimensions of a transposed convolution (sometimes called deconvolution) follow this formula:\n",
    "\n",
    "output_size = (input_size - 1) * stride - 2 * padding + kernel_size\n",
    "\n",
    "For our first layer (latent_dim → 512):\n",
    "- Input: 1×1\n",
    "- Calculation: (1-1)*1 - 2*0 + 4 = 4\n",
    "- Output: 4×4\n",
    "\n",
    "For subsequent layers with stride=2, padding=1:\n",
    "- Input: H×W\n",
    "- Calculation: (H-1)*2 - 2*1 + 4 = 2H + 2\n",
    "- This doubles the spatial dimensions each time\n",
    "\n",
    "#### Key Design Elements\n",
    "\n",
    "- **Transposed Convolutions**: Learnable upsampling operations that increase spatial dimensions while decreasing channel depth\n",
    "- **ReLU Activations**: Prevent vanishing gradients in deep networks\n",
    "- **BatchNorm**: Stabilizes training by normalizing activations\n",
    "- **Tanh Output**: Maps final pixel values to [-1,1] range, appropriate for normalized image data\n",
    "- **Symmetric Architecture**: The generator's architecture mirrors the discriminator but in reverse\n",
    "\n",
    "#### Why This Architecture Works\n",
    "\n",
    "- **Progressive Upsampling**: Gradually increases spatial dimensions while decreasing feature channels\n",
    "- **No Fully Connected Layers**: Maintains spatial information throughout\n",
    "- **Fixed Kernel Size**: Consistent 4×4 kernels across all layers\n",
    "- **No Bias Terms**: Omitted where batch normalization is applied\n",
    "- **Decreasing Feature Maps**: 512→256→128→64→3 channels mirrors discriminator's increasing pattern\n",
    "\n",
    "#### Usage Example\n",
    "\n",
    "```python\n",
    "latent_dim = 100\n",
    "G = Generator(latent_dim)\n",
    "z = torch.randn(32, latent_dim, 1, 1)  # Batch of 32 random latent vectors\n",
    "fake_images = G(z)  # Shape: (32, 3, 64, 64)\n",
    "```\n",
    "\n",
    "This shows:\n",
    "1. **Progressive spatial expansion** from 1×1 to 64×64\n",
    "2. **Channel depth reduction** from latent_dim to 3 (RGB)\n",
    "3. **Systematic architecture** ensuring proper dimensionality transformations\n",
    "4. **Consistent batch dimension** (N) throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dc215",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:42.729961Z",
     "start_time": "2024-11-01T12:55:42.726282Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(Module):\n",
    "    \"\"\"DCGAN-style generator that transforms latent vectors into images.\n",
    "    \n",
    "    ## Architecture Overview\n",
    "    - **Input**: `(N, latent_dim, 1, 1)` random noise tensor  \n",
    "    - **Output**: `(N, 3, 64, 64)` generated image (RGB, normalized to [-1,1])  \n",
    "    - **Key Components**:\n",
    "      - 5 transposed convolutional layers (nn.ConvTranspose2d)\n",
    "      - Batch normalization after each layer (except output)\n",
    "      - ReLU activation (except output layer using Tanh)\n",
    "      - Progressive upsampling from 1x1 → 4x4 → ... → 64x64\n",
    "\n",
    "    ## Layer-by-Layer Breakdown\n",
    "\n",
    "    1. **Projection Layer**  \n",
    "       - Input: `latent_dim x 1 x 1`  \n",
    "       - ConvT2d: `latent_dim → 512`, kernel=4, stride=1, padding=0  \n",
    "       - Output: `512 x 4 x 4` (initial feature map)  \n",
    "\n",
    "    2. **Upsample Block 1**  \n",
    "       - ConvT2d: `512 → 256`, kernel=4, stride=2, padding=1  \n",
    "       - Output: `256 x 8 x 8`  \n",
    "\n",
    "    3. **Upsample Block 2**  \n",
    "       - ConvT2d: `256 → 128`, kernel=4, stride=2, padding=1  \n",
    "       - Output: `128 x 16 x 16`  \n",
    "\n",
    "    4. **Upsample Block 3**  \n",
    "       - ConvT2d: `128 → 64`, kernel=4, stride=2, padding=1  \n",
    "       - Output: `64 x 32 x 32`  \n",
    "\n",
    "    5. **Output Layer**  \n",
    "       - ConvT2d: `64 → 3`, kernel=4, stride=2, padding=1  \n",
    "       - Tanh(): Normalizes to [-1, 1]  \n",
    "       - Output: `3 x 64 x 64` (RGB image)  \n",
    "\n",
    "    ## Dimensionality Progression\n",
    "    ```python\n",
    "    latent_dimx1x1 → 512x4x4 → 256x8x8 → 128x16x16 → 64x32x32 → 3x64x64\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        latent_dim (int): Size of the input noise vector (typically 100)\n",
    "\n",
    "    Example:\n",
    "        >>> G = Generator(latent_dim=100)\n",
    "        >>> noise = torch.randn(32, 100, 1, 1)  # Batch of 32 noise vectors\n",
    "        >>> generated_images = G(noise)  # Shape: (32, 3, 64, 64)\n",
    "\n",
    "\n",
    "    #### Shape Transformation Diagram\n",
    "\n",
    "        Input Vector:       [N, latent_dim, 1, 1]\n",
    "        │\n",
    "        ▼\n",
    "        Layer 1 ConvT:      [N, 512, 4, 4]      # (1-1)*1 - 2*0 + 4 = 4\n",
    "        │\n",
    "        ▼\n",
    "        Layer 2 ConvT:      [N, 256, 8, 8]      # (4-1)*2 - 2*1 + 4 = 8\n",
    "        │\n",
    "        ▼  \n",
    "        Layer 3 ConvT:      [N, 128, 16, 16]    # (8-1)*2 - 2*1 + 4 = 16\n",
    "        │\n",
    "        ▼\n",
    "        Layer 4 ConvT:      [N, 64, 32, 32]     # (16-1)*2 - 2*1 + 4 = 32\n",
    "        │\n",
    "        ▼\n",
    "        Output ConvT:       [N, 3, 64, 64]      # (32-1)*2 - 2*1 + 4 = 64\n",
    "\n",
    "\n",
    "        [latent_dim,1,1]         [512,4,4]              [256,8,8]\n",
    "        ┌───────────────┐        ┌───────────┐         ┌──────────┐\n",
    "        │ Latent Vector │        │  512 fm   │         │  256 fm  │\n",
    "        │     1×1       │─ConvT─▶│   4×4     │─ConvT──▶│    8×8   │\n",
    "        └───────────────┘        └───────────┘         └──────────┘\n",
    "                                        \n",
    "        [128,16,16]             [64,32,32]           [3,64,64]\n",
    "        ┌──────────────┐        ┌──────────────┐      ┌──────────┐\n",
    "        │   128 fm     │        │    64 fm     │      │   RGB    │\n",
    "        │   16×16      │─ConvT─▶│    32×32     │─ConvT▶│  Image   │\n",
    "        └──────────────┘        └──────────────┘      └──────────┘\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: latent_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 512 x 4 x 4\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 256 x 8 x 8\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 128 x 16 x 16\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # 64 x 32 x 32\n",
    "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: 3 x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8b0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:45.057100Z",
     "start_time": "2024-11-01T12:55:45.037334Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# run this cell to verify your generator implementation\n",
    "# latent_dim = 128\n",
    "latent_dim = 256\n",
    "generator = Generator(latent_dim)\n",
    "tests.check_generator(generator, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24419370",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "In the following section, we create the optimizers for the generator and discriminator. You may want to experiment with different optimizers, learning rates and other hyperparameters as they tend to impact the output quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc1354",
   "metadata": {},
   "source": [
    "### Exercise: implement the optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cad8480",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:46.945287Z",
     "start_time": "2024-11-01T12:55:46.942743Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def create_optimizers(generator: Module, discriminator: Module):\n",
    "    \"\"\"Creates optimizers for generator and discriminator\"\"\"\n",
    "    \n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))  # Lowered discriminator LR\n",
    "\n",
    "    return g_optimizer, d_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575998b1",
   "metadata": {},
   "source": [
    "## Losses implementation\n",
    "\n",
    "In this section, we are going to implement the loss function for the generator and the discriminator. You can and should experiment with different loss function.\n",
    "\n",
    "Some tips:\n",
    "* You can choose the commonly used the binary cross entropy loss or select other losses we have discovered in the course, such as the Wasserstein distance.\n",
    "* You may want to implement a gradient penalty function as discussed in the course. It is not required and the code will work whether you implement it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d2ad",
   "metadata": {},
   "source": [
    "### Exercise: implement the generator loss\n",
    "\n",
    "The generator's goal is to get the discriminator to think its generated images (= \"fake\" images) are real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77da2882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:49.962341Z",
     "start_time": "2024-11-01T12:55:49.960279Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_logits):\n",
    "    \"\"\"Generator loss\"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    # Generate labels for fake images as if they were real\n",
    "    labels = torch.ones_like(fake_logits)\n",
    "    return criterion(fake_logits, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb1ac9",
   "metadata": {},
   "source": [
    "### Exercise: implement the discriminator loss\n",
    "\n",
    "We want the discriminator to give high scores to real images and low scores to fake ones and the discriminator loss should reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5970f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:51.793843Z",
     "start_time": "2024-11-01T12:55:51.791608Z"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "    \"\"\"Discriminator loss\"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    # Real images should be labeled as 1\n",
    "    real_labels = torch.ones_like(real_logits)\n",
    "    # Fake images should be labeled as 0\n",
    "    fake_labels = torch.zeros_like(fake_logits)\n",
    "\n",
    "    real_loss = criterion(real_logits, real_labels)\n",
    "    fake_loss = criterion(fake_logits, fake_labels)\n",
    "\n",
    "    return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d48d8",
   "metadata": {},
   "source": [
    "### Exercise (Optional): Implement the gradient Penalty\n",
    "\n",
    "In the course, we discussed the importance of gradient penalty in training certain types of Gans. Implementing this function is not required and depends on some of the design decision you made (discriminator architecture, loss functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d6e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    \"\"\"\n",
    "    Calculate the gradient penalty loss for WGAN GP\n",
    "    Args:\n",
    "        discriminator: the discriminator model\n",
    "        real_samples: real images from dataset\n",
    "        fake_samples: generated images from generator\n",
    "    Returns:\n",
    "        gradient penalty value\n",
    "    \"\"\"\n",
    "    alpha = torch.rand((real_samples.size(0), 1, 1, 1)).to(real_samples.device)\n",
    "    \n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    \n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates).to(real_samples.device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2b354",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "Training will involve alternating between training the discriminator and the generator. You'll use your functions real_loss and fake_loss to help you calculate the discriminator losses.\n",
    "\n",
    "* You should train the discriminator by alternating on real and fake images\n",
    "* Then the generator, which tries to trick the discriminator and should have an opposing loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d61925",
   "metadata": {},
   "source": [
    "### Exercise: implement the generator step and the discriminator step functions\n",
    "\n",
    "Each function should do the following:\n",
    "* calculate the loss\n",
    "* backpropagate the gradient\n",
    "* perform one optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc72aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:55.603692Z",
     "start_time": "2024-11-01T12:55:55.600386Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_step(generator, discriminator, g_optimizer, batch_size, latent_dim, device):\n",
    "    \"\"\"One training step for generator\"\"\"\n",
    "    g_optimizer.zero_grad()\n",
    "\n",
    "    # Generate fake images\n",
    "    noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "    fake_images = generator(noise)\n",
    "\n",
    "    # Get discriminator output on fake images\n",
    "    fake_logits = discriminator(fake_images)\n",
    "\n",
    "    # Calculate generator loss\n",
    "    g_loss = generator_loss(fake_logits)\n",
    "\n",
    "    # Backpropagate and update\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    return {'loss': g_loss}\n",
    "\n",
    "\n",
    "def discriminator_step(generator, discriminator, d_optimizer, batch_size, latent_dim, real_images, device):\n",
    "    \"\"\"One training step for discriminator\"\"\"\n",
    "    d_optimizer.zero_grad()\n",
    "\n",
    "    # Get discriminator output on real images\n",
    "    real_logits = discriminator(real_images)\n",
    "\n",
    "    # Generate and get output on fake images\n",
    "    noise = torch.randn(batch_size, latent_dim, 1, 1, device=device)\n",
    "    with torch.no_grad():\n",
    "        fake_images = generator(noise)\n",
    "    fake_logits = discriminator(fake_images)\n",
    "\n",
    "    # Calculate discriminator loss\n",
    "    d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "\n",
    "    # Backpropagate and update\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "    return {'loss': d_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c39a6",
   "metadata": {},
   "source": [
    "### Main training loop\n",
    "\n",
    "You don't have to implement anything here but you can experiment with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd95fff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:55:59.531337Z",
     "start_time": "2024-11-01T12:55:59.528499Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 256 \n",
    "\n",
    "# Use for Apple MPS \n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'  \n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'  \n",
    "else:\n",
    "    device = 'cpu'  \n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "n_epochs = 50 \n",
    "\n",
    "# Larger batch sizes stabilize training (reduce if memory is limited)\n",
    "batch_size = 128  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69af6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:56:01.815696Z",
     "start_time": "2024-11-01T12:56:01.607188Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print_every = 50\n",
    "\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer, d_optimizer = create_optimizers(generator, discriminator)\n",
    "\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=64,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        drop_last=True,\n",
    "                        pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2b5d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T12:56:10.656590Z",
     "start_time": "2024-11-01T12:56:10.653459Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def display(fixed_latent_vector: torch.Tensor):\n",
    "    \"\"\" helper function to display images during training \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    plot_size = 16\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size / 2), idx + 1, xticks=[], yticks=[])\n",
    "        img = fixed_latent_vector[idx, ...].detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = denormalize(img)\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a770c",
   "metadata": {},
   "source": [
    "### Exercise: implement the training strategy\n",
    "\n",
    "You should experiment with different training strategies. For example:\n",
    "\n",
    "* train the generator more often than the discriminator. \n",
    "* added noise to the input image\n",
    "* use label smoothing\n",
    "\n",
    "Implement with your training strategy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b569d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T14:38:18.811489Z",
     "start_time": "2024-11-01T12:56:16.466460Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Fixed latent vector for visualizing generator's progress\n",
    "fixed_latent_vector = torch.randn(16, latent_dim, 1, 1).to(device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Define the learning rate schedulers\n",
    "d_scheduler = StepLR(d_optimizer, step_size=30, gamma=0.5)  # Reduce LR every 30 epochs\n",
    "g_scheduler = StepLR(g_optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Starting Epoch [{epoch + 1}/{n_epochs}]...\")  # Start of each epoch\n",
    "\n",
    "    for batch_i, real_images in enumerate(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        ####################################\n",
    "        # Train the discriminator\n",
    "        d_loss = discriminator_step(\n",
    "            generator, discriminator, d_optimizer,\n",
    "            batch_size, latent_dim, real_images, device\n",
    "        )\n",
    "\n",
    "        # Train the generator twice for better convergence\n",
    "        g_loss = generator_step(\n",
    "            generator, discriminator, g_optimizer,\n",
    "            batch_size, latent_dim, device\n",
    "        )\n",
    "        g_loss = generator_step(\n",
    "            generator, discriminator, g_optimizer,\n",
    "            batch_size, latent_dim, device\n",
    "        )\n",
    "        ####################################\n",
    "\n",
    "        # Append discriminator and generator losses for tracking\n",
    "        d = d_loss['loss'].item()\n",
    "        g = g_loss['loss'].item()\n",
    "        losses.append((d, g))\n",
    "\n",
    "        # Print loss details only at the end of the epoch\n",
    "        if batch_i == len(dataloader) - 1:\n",
    "            time = str(datetime.now()).split('.')[0]\n",
    "            print(\n",
    "                f'End of Epoch [{epoch + 1}/{n_epochs}]. {time} | Epoch [{epoch + 1}/{n_epochs}] | Batch {batch_i}/{len(dataloader)} | d_loss: {d:.4f} | g_loss: {g:.4f}')\n",
    "\n",
    "    # Display images during training\n",
    "    generator.eval()\n",
    "    generated_images = generator(fixed_latent_vector)\n",
    "    display(generated_images)\n",
    "    generator.train()\n",
    "\n",
    "    # Step the learning rate schedulers after each epoch\n",
    "    d_scheduler.step()\n",
    "    g_scheduler.step()\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9860ab0f",
   "metadata": {},
   "source": [
    "### Training losses\n",
    "\n",
    "Plot the training losses for the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa251a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T14:53:43.902624Z",
     "start_time": "2024-11-01T14:53:43.632371Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(figsize=(16, 4))\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82375a58b9d80580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T03:32:12.550134Z",
     "start_time": "2024-11-02T03:32:10.809443Z"
    }
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to html dlnd_face_generation_starter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb189711",
   "metadata": {},
   "source": [
    "### Question: What do you notice about your generated samples and how might you improve this model?\n",
    "When you answer this question, consider the following factors:\n",
    "* The dataset is biased; it is made of \"celebrity\" faces that are mostly white\n",
    "* Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "* Optimization strategy; optimizers and number of epochs affect your final result\n",
    "* Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44829483",
   "metadata": {},
   "source": [
    "**Answer:** The generated faces show reasonable structure but lack fine details and diversity, with noticeable artifacts and blurriness. The training graphs indicate instability in generator performance (high fluctuating loss) while the discriminator remains relatively stable. The model could be improved by increasing model capacity, implementing WGAN-GP loss with gradient penalty, using progressive growing architecture, and most importantly, training for more epochs with a more diverse dataset to address the current bias towards white celebrities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77ed9d",
   "metadata": {},
   "source": [
    "### Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_face_generation.ipynb\".  \n",
    "\n",
    "Submit the notebook using the ***SUBMIT*** button in the bottom right corner of the Project Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd0251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
