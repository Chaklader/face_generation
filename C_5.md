<br>
<br>

# C-5: Modern GANs

<br>
<br>

1. Advanced GAN Loss Functions

    - Limitations of Binary Cross-Entropy Loss
    - Wasserstein Distance as a Loss Function
    - Understanding Lipschitz Continuity
    - Gradient Penalty Implementation

2. Progressive Growing of GANs (ProGAN)

    - Incremental Resolution Enhancement
    - Layer Fading Methodology
    - Minibatch Discrimination Techniques
    - Normalization and Stabilization Strategies

3. StyleGAN Architecture

    - Mapping Network and Latent Space Disentanglement
    - Noise Injection for Stochastic Variation
    - Adaptive Instance Normalization (AdaIN)
    - Style Mixing and Transfer Capabilities

4. Practical Implementation Considerations
    - Choosing the Right GAN Architecture
    - Addressing Mode Collapse
    - Balancing Training Stability and Image Quality
    - Application-Specific Optimization Strategies

#### Advanced GAN Loss Functions

##### Limitations of Binary Cross-Entropy Loss

The Binary Cross-Entropy (BCE) loss function served as the foundation for the original GAN formulation, but it
introduces several significant challenges that make training unstable and often frustrating. Understanding these
limitations helps explain why researchers developed alternative loss functions for GANs.

At its core, the BCE loss creates a minimax game between the generator and discriminator:

$$\min_G \max_D V(D, G) = \mathbb{E}*{x \sim p*{data}}[\log D(x)] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))]$$

This formulation leads to several problematic training dynamics. The first major issue is the vanishing gradient
problem. When the discriminator becomes too successful at its job—confidently identifying generated samples as fake—the
gradient of the loss with respect to the generator's parameters approaches zero:

$$\nabla_{\theta_G} \log(1 - D(G(z))) \approx 0 \text{ when } D(G(z)) \approx 0$$

This essentially means the generator stops receiving meaningful feedback about how to improve. It's like trying to learn
a skill when your teacher only says "that's wrong" without explaining why or how to fix it. The learning process stalls
because the generator has no direction for improvement.

To partially address this issue, many implementations use a non-saturating version of the loss where the generator tries
to maximize $\log(D(G(z)))$ instead of minimizing $\log(1 - D(G(z)))$. This provides stronger gradients when the
generator is performing poorly, but doesn't solve all the problems with BCE loss.

Another significant limitation is mode collapse. Real-world data distributions typically have multiple modes—distinct
clusters or types of examples. For instance, a dataset of handwritten digits contains ten different modes (one for each
digit). Mode collapse occurs when the generator produces only a limited subset of these modes, ignoring the full
diversity of the target distribution. This happens because BCE loss only rewards fooling the discriminator; if the
generator finds a few outputs that consistently work, it has no incentive to explore other possibilities.

The instability of BCE loss also stems from its mathematical properties. When the distributions of real and generated
data have minimal overlap (as is often the case early in training), the discriminator can achieve near-perfect
separation. This creates a binary feedback signal that fluctuates dramatically as training progresses, resulting in
oscillating behavior rather than smooth convergence.

Furthermore, BCE loss doesn't provide a meaningful measure of distance between the real and generated distributions. It
only indicates whether the discriminator can distinguish between them, without quantifying how different they are. This
makes it difficult to track progress during training, as the loss values don't correlate well with the perceptual
quality of generated samples.

These limitations become especially problematic when generating high-resolution images or working with complex data
distributions. The training dynamics can lead to situations where:

1. The discriminator becomes too powerful too quickly, providing minimal guidance to the generator
2. The generator focuses on exploiting weaknesses in the discriminator rather than learning the true data distribution
3. Training becomes unstable, with performance oscillating or degrading after initial progress
4. Generated outputs lack diversity, failing to capture the full range of the target distribution

These issues motivated researchers to explore alternative loss functions that could provide more stable training
dynamics and better quality outputs. Among these alternatives, the Wasserstein distance emerged as a particularly
effective approach, addressing many of the core limitations of BCE loss.

##### Wasserstein Distance as a Loss Function

The Wasserstein distance, also known as the Earth Mover's Distance (EMD), represents a significant advancement in GAN
training methodology. Unlike BCE loss, which struggles to provide meaningful gradients when distributions have minimal
overlap, the Wasserstein distance offers a smooth measure of similarity between distributions even when they don't share
significant support.

To understand the Wasserstein distance intuitively, imagine each probability distribution as piles of earth, where the
height at each point represents the probability density. The Wasserstein distance measures the minimum amount of "work"
(earth × distance) needed to transform one pile into the other. This provides a meaningful metric even when
distributions don't overlap, as it considers how far apart they are, not just whether they're different.

Mathematically, the Wasserstein distance between two probability distributions P and Q is defined as:

$$W(P, Q) = \inf_{\gamma \in \Pi(P, Q)} \mathbb{E}_{(x,y) \sim \gamma}[||x - y||]$$

Where $\Pi(P, Q)$ is the set of all joint distributions whose marginals are P and Q. Computing this directly is
computationally intractable for high-dimensional distributions, but the Kantorovich-Rubinstein duality provides a more
practical formulation:

$$W(P, Q) = \sup_{||f||*L \leq 1} \mathbb{E}*{x \sim P}[f(x)] - \mathbb{E}_{x \sim Q}[f(x)]$$

Where the supremum is taken over all 1-Lipschitz functions f. This formulation leads directly to the Wasserstein GAN
objective:

$$\min_G \max_{C: ||C||*L \leq 1} \mathbb{E}*{x \sim p_{data}}[C(x)] - \mathbb{E}_{z \sim p_z}[C(G(z))]$$

In this formulation, the discriminator is replaced by a critic (C) that no longer classifies inputs as real or fake but
instead assigns scalar scores. The critic tries to maximize the difference between the scores assigned to real data and
generated data, while the generator tries to minimize this difference. Importantly, the critic must satisfy the
Lipschitz continuity constraint (which we'll explore in the next section).

The Wasserstein loss offers several substantial advantages over BCE:

1. **Meaningful gradients everywhere**: Even when the real and generated distributions don't overlap, the Wasserstein
   distance provides meaningful gradients that indicate how to improve the generator. This addresses the vanishing
   gradient problem that plagues standard GANs.
2. **Correlation with perceptual quality**: The Wasserstein distance correlates well with the perceived quality of
   generated samples. As the distance decreases, the generated samples typically become more realistic, providing a
   useful metric for tracking training progress.
3. **More stable training dynamics**: The smoother nature of the Wasserstein distance leads to more stable training,
   with less oscillation and mode collapse. The critic can be trained to optimality without crippling the generator,
   unlike in standard GANs where a perfect discriminator leads to vanishing gradients.
4. **Theoretical guarantees**: Under ideal conditions, minimizing the Wasserstein distance guarantees convergence to the
   true data distribution, providing stronger theoretical support than the original GAN formulation.

The practical implementation of Wasserstein GANs differs from standard GANs in several key ways:

- The critic outputs unbounded scalar values rather than probabilities, so there's no sigmoid activation at the output
- The loss functions are simpler, with no logarithms:
    - Critic loss: $\mathbb{E}[C(x)] - \mathbb{E}[C(G(z))]$
    - Generator loss: $-\mathbb{E}[C(G(z))]$
- The critic must satisfy the Lipschitz continuity constraint
- Training typically involves multiple critic updates per generator update to ensure the critic approximates the
  Wasserstein distance well

These changes result in more stable training and higher quality outputs, particularly for complex data distributions and
high-resolution images. The Wasserstein distance essentially provides a more informative and smoother "compass" that
guides the generator toward the true data distribution, even when starting from a point far from the target.

##### Understanding Lipschitz Continuity

Lipschitz continuity represents a crucial mathematical constraint in Wasserstein GANs that ensures the critic provides
meaningful and bounded feedback to the generator. This concept might initially seem abstract, but it plays a fundamental
role in the stability and effectiveness of WGAN training.

A function f is said to be K-Lipschitz continuous if there exists a real constant K ≥ 0 such that for all x₁ and x₂ in
the domain:

$$|f(x_1) - f(x_2)| \leq K|x_1 - x_2|$$

In simpler terms, this means the function cannot change too rapidly—the change in the output is bounded by K times the
change in the input. When K = 1, we call this 1-Lipschitz continuity, which is the constraint required for Wasserstein
GANs.

For the critic function C in a WGAN, the 1-Lipschitz constraint means:

$$|C(x_1) - C(x_2)| \leq |x_1 - x_2|$$

This constraint serves several essential purposes in the context of WGANs:

1. **Theoretical necessity**: The Kantorovich-Rubinstein duality, which allows the practical computation of the
   Wasserstein distance, requires the critic to be 1-Lipschitz. Without this constraint, the critic would not be
   computing a proper approximation of the Wasserstein distance.
2. **Preventing extreme gradients**: Without the Lipschitz constraint, the critic could assign arbitrarily large
   differences in scores between real and generated samples, leading to explosive gradients and unstable training.
3. **Enforcing a smooth critic landscape**: The constraint ensures that the critic's gradient with respect to its inputs
   has a norm bounded by 1, creating a smoother optimization landscape for the generator to navigate.

The 1-Lipschitz constraint can be understood geometrically as limiting the norm of the gradient of the critic function:

$$||\nabla_x C(x)||_2 \leq 1$$

This means that for any input, the gradient of the critic function cannot exceed a magnitude of 1. This constraint
ensures that small changes in the input lead to proportionally small changes in the output, preventing the critic from
creating "steep cliffs" in the function landscape that could lead to training instability.

In the original WGAN paper, the authors enforced this constraint through weight clipping—restricting all weights in the
critic network to a small range (e.g., [-0.01, 0.01]). While this approach works to some extent, it has significant
limitations:

1. Finding the appropriate clipping threshold requires careful tuning
2. Excessive clipping can cause the critic to learn oversimplified functions
3. Weight clipping can lead to vanishing or exploding gradients in deep networks
4. The constraint affects all directions equally, which may be too restrictive

The practical implementation of weight clipping involves a simple operation after each gradient update:

```python
for param in critic.parameters():
    param.data.clamp_(-c, c)  # Clip weights to range [-c, c]
```

Where `c` is the clipping threshold (typically a small value like 0.01).

However, weight clipping proved to be a somewhat crude approximation of Lipschitz continuity. It forces the critic to
learn functions that are often simpler than needed, potentially limiting its ability to approximate the Wasserstein
distance accurately. This limitation motivated the development of more sophisticated approaches to enforcing Lipschitz
continuity, most notably the gradient penalty method discussed in the next section.

Understanding Lipschitz continuity helps clarify why WGANs represent a theoretical improvement over standard GANs. By
constraining the critic to compute a proper distance metric between distributions, WGANs provide more stable and
informative training signals to the generator, allowing it to more effectively learn complex data distributions without
suffering from issues like mode collapse or vanishing gradients.

The Lipschitz constraint essentially creates a balanced playing field between the generator and critic, preventing
either from overwhelming the other while ensuring that their adversarial game leads to meaningful learning rather than
degenerate solutions or instability.

##### Gradient Penalty Implementation

The gradient penalty approach, introduced in the WGAN-GP paper, represents a more sophisticated and effective method for
enforcing the Lipschitz continuity constraint compared to weight clipping. This technique directly penalizes the
gradient norm of the critic, encouraging it to maintain 1-Lipschitz continuity without the limitations imposed by weight
clipping.

The core insight behind gradient penalty is to directly enforce the condition that the gradient norm of the critic
should be 1 for points sampled from the data distribution and the generator distribution. Since checking all possible
points is infeasible, the method samples points along straight lines between pairs of real and generated data points.
This sampling strategy is motivated by the observation that optimal critics will have gradients with norm 1 almost
everywhere along these lines.

Mathematically, the gradient penalty is formulated as:

$$\lambda \mathbb{E}*{\hat{x} \sim \mathbb{P}*{\hat{x}}}[(||\nabla_{\hat{x}} C(\hat{x})||_2 - 1)^2]$$

Where:

- $\hat{x}$ represents points sampled along straight lines between real and generated samples
- $\lambda$ is the penalty coefficient (typically set to 10)
- $||\nabla_{\hat{x}} C(\hat{x})||_2$ is the L2 norm of the gradient of the critic with respect to its input

The sampling of interpolated points follows:

$$\hat{x} = \alpha x + (1 - \alpha) G(z)$$

Where $\alpha$ is randomly sampled from a uniform distribution U[0,1] for each pair of real sample x and generated
sample G(z).

The complete WGAN-GP objective then becomes:

$$L = \mathbb{E}*{z \sim p_z}[C(G(z))] - \mathbb{E}*{x \sim p_{data}}[C(x)] + \lambda \mathbb{E}*{\hat{x} \sim \mathbb{P}*{\hat{x}}}[(||\nabla_{\hat{x}} C(\hat{x})||_2 - 1)^2]$$

For the critic to minimize, and:

$$L_G = -\mathbb{E}_{z \sim p_z}[C(G(z))]$$

For the generator to minimize.

Implementing the gradient penalty requires computing gradients of the critic's output with respect to its inputs, which
involves some specific technical considerations in deep learning frameworks. Here's how it's typically implemented in
PyTorch:

```python
def compute_gradient_penalty(critic, real_samples, fake_samples, device):
    # Random interpolation coefficient for each sample in the batch
    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)

    # Create interpolated samples
    interpolates = alpha * real_samples + (1 - alpha) * fake_samples
    interpolates.requires_grad_(True)  # Enable gradient computation

    # Forward pass through critic
    critic_interpolates = critic(interpolates)

    # Create dummy tensor of ones for gradient computation
    ones = torch.ones_like(critic_interpolates, device=device, requires_grad=False)

    # Compute gradients of critic output with respect to interpolated inputs
    gradients = torch.autograd.grad(
        outputs=critic_interpolates,
        inputs=interpolates,
        grad_outputs=ones,
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    # Calculate gradient penalty: ((||∇C(x̂)||₂ - 1)²)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()

    return gradient_penalty
```

This implementation is then integrated into the critic's training loop:

```python
# Critic training iteration
for _ in range(n_critic):  # Typically 5 critic updates per generator update
    # Train with real samples
    real_output = critic(real_samples)

    # Train with fake samples
    with torch.no_grad():  # No need to compute generator gradients
        fake_samples = generator(noise)
    fake_output = critic(fake_samples)

    # Compute gradient penalty
    gp = compute_gradient_penalty(critic, real_samples, fake_samples, device)

    # Wasserstein distance estimate
    wasserstein_distance = fake_output.mean() - real_output.mean()

    # Critic loss with gradient penalty
    critic_loss = wasserstein_distance + lambda_gp * gp

    # Optimize critic
    critic_optimizer.zero_grad()
    critic_loss.backward()
    critic_optimizer.step()
```

The gradient penalty approach offers several advantages over weight clipping:

1. **More expressive critic functions**: The critic can learn more complex functions without the restrictive effect of
   weight clipping, allowing for better approximation of the Wasserstein distance.
2. **Improved stability**: Gradient penalty provides smoother training dynamics with fewer oscillations than weight
   clipping.
3. **Better performance**: WGANs with gradient penalty consistently produce higher quality outputs and show better
   convergence properties across a range of datasets and architectures.
4. **No hyperparameter search for clipping threshold**: While gradient penalty introduces its own hyperparameter
   $\lambda$, its optimal value (around 10) seems to work well across many different problems, unlike the clipping
   threshold which often requires task-specific tuning.
5. **Compatibility with normalization techniques**: Gradient penalty works well with batch normalization, layer
   normalization, and other techniques that might interact poorly with weight clipping.

One important implementation detail is that batch normalization should typically be avoided in the critic when using
gradient penalty. This is because batch normalization makes the critic's output dependent on other samples in the batch,
which complicates the gradient computation. Instance normalization or layer normalization serve as effective
alternatives.

The gradient penalty approach has become the standard method for enforcing Lipschitz continuity in Wasserstein GANs,
enabling more stable training and higher quality results across a wide range of applications. This innovation, combined
with the theoretical advantages of the Wasserstein distance, has made WGAN-GP a powerful and widely adopted framework
for generative modeling.
