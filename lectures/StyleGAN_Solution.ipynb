{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### StyleGan\n",
    "\n",
    "Look at this: [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/) ! Would you believe this picture is not real??? Well it is not! Feel free to refresh the page to look at more examples. This picture has been generated by [StyleGan](https://arxiv.org/pdf/1812.04948.pdf), a groundbreaking architecture in the world of GANs! \n",
    "\n",
    "The StyleGan uses a lot of tricks that we have seen in the course (eg, progressive growing) but also relies on a novel approach to controlled generation. The figure below describes the architecture of the network:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src='../images/stylegan.png' width=60% />\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src='../images/style_gan.png' width=60% />\n",
    "</p>\n",
    "\n",
    "Wow! We have a completely new network in our GAN. This network is called the **noise mapping** network. The architecture of this network is fairly simple, it only consists of 8 fully connected layers. The authors argue that mapping the latent vector $z$ to a new latent vector $w$ facilitates **disentanglement**. \n",
    "\n",
    "We talked about disentanglement in the course: when modifying the latent vector $z$ to control the aspect of the generated image, we often face entangled features. For example, longer hair can be correlated with a more feminine face. Using the mapping vector in StyleGan facilities the decorrelation of such features.\n",
    "\n",
    "What happens next? The generated $w$ latent vector is injected into a \"classic\" generator network. However, this network has two components you are not yet familiar with, as seen in the figure above. Indeed, after each convolution layer, we see that the authors are adding **noise**. Moreover, the convolution output with added noise is then fed into a **adaptive instance normalization layer or AdaIN**.\n",
    "\n",
    "In this notebook, you will implement a **noise injection layer** and the **AdaIn layer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090e643",
   "metadata": {},
   "source": [
    "---\n",
    "#### StyleGAN: Revolutionizing High-Quality Image Generation\n",
    "\n",
    "StyleGAN represents one of the most significant breakthroughs in generative adversarial networks, fundamentally changing how we think about image generation and controllable synthesis. To understand why StyleGAN matters, imagine being an artist who can not only create incredibly realistic portraits but also precisely control every aspect of the subject's appearance—from their age and gender to subtle details like hair texture and facial expression. StyleGAN achieves exactly this level of control in the digital realm.\n",
    "\n",
    "The key insight behind StyleGAN lies in recognizing that traditional GANs treat all aspects of image generation equally, mixing low-level details like texture with high-level features like face shape in an entangled manner. StyleGAN introduces a novel architecture that disentangles these different aspects, allowing unprecedented control over the generation process.\n",
    "\n",
    "##### The Architecture Revolution: From Noise to Style\n",
    "\n",
    "Traditional GANs begin with a random noise vector that gets transformed through a series of layers until it becomes an image. This approach has a fundamental limitation: the initial noise vector must encode everything about the final image, from global structure to fine details, in a single compressed representation. StyleGAN takes a radically different approach by introducing a mapping network and style-based generation.\n",
    "\n",
    "##### The Mapping Network: Creating the Style Space\n",
    "\n",
    "The first major innovation in StyleGAN is the mapping network, which transforms the input noise vector $z$ into an intermediate latent code $w$. This mapping network consists of several fully connected layers that learn to map from the noise space $\\mathcal{Z}$ to what the authors call the style space $\\mathcal{W}$.\n",
    "\n",
    "This transformation serves a crucial purpose. The original noise space $\\mathcal{Z}$ typically follows a simple distribution like a multivariate Gaussian, but the actual distribution of meaningful images is far from Gaussian. The mapping network learns to warp the simple input distribution into a more suitable intermediate representation where different dimensions correspond to meaningful and disentangled factors of variation.\n",
    "\n",
    "The mapping network can be expressed mathematically as:\n",
    "$$w = f(z)$$\n",
    "where $f$ is an 8-layer multilayer perceptron that maps from $\\mathcal{Z}$ to $\\mathcal{W}$.\n",
    "\n",
    "##### Style-Based Generation: Adaptive Instance Normalization\n",
    "\n",
    "The second revolutionary component is the style-based generator architecture. Instead of feeding the latent code directly into the generator layers, StyleGAN uses the intermediate latent code $w$ to control the generation process through Adaptive Instance Normalization (AdaIN).\n",
    "\n",
    "Each generator layer receives its style information from a learned affine transformation of $w$. The AdaIN operation normalizes the feature maps and then applies style-specific scaling and shifting:\n",
    "\n",
    "$$\\text{AdaIN}(x_i, y) = y_{s,i} \\frac{x_i - \\mu(x_i)}{\\sigma(x_i)} + y_{b,i}$$\n",
    "\n",
    "where $x_i$ is the feature map of the $i$-th channel, $y_{s,i}$ and $y_{b,i}$ are the style-derived scaling and bias parameters, and $\\mu(x_i)$ and $\\sigma(x_i)$ are the mean and standard deviation of $x_i$.\n",
    "\n",
    "This approach allows different layers to be controlled by different style vectors, enabling fine-grained control over different aspects of the generated image. Coarse layers control high-level features like face shape and pose, while fine layers control details like hair texture and skin pores.\n",
    "\n",
    "##### The Complete StyleGAN Architecture\n",
    "\n",
    "Let me illustrate how all these components work together in the complete StyleGAN architecture:\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Z[\"Random Noise Vector<br>z ~ N(0,1)<br>512-dimensional\"] --> MappingNet[\"Mapping Network<br>8 FC layers<br>z → w transformation\"]\n",
    "    \n",
    "    MappingNet --> W[\"Style Vector w<br>512-dimensional<br>Intermediate latent space\"]\n",
    "    \n",
    "    W --> A1[\"Affine Transform A1<br>w → (scale₁, bias₁)\"]\n",
    "    W --> A2[\"Affine Transform A2<br>w → (scale₂, bias₂)\"]\n",
    "    W --> A3[\"Affine Transform A3<br>w → (scale₃, bias₃)\"]\n",
    "    W --> ADots[\"... more transforms\"]\n",
    "    W --> An[\"Affine Transform An<br>w → (scaleₙ, biasₙ)\"]\n",
    "    \n",
    "    Const[\"Learned Constant<br>4×4×512 tensor\"] --> Conv1[\"Conv Layer 1<br>3×3 convolution\"]\n",
    "    Conv1 --> AdaIN1[\"AdaIN Layer 1<br>Normalization + Style\"]\n",
    "    A1 --> AdaIN1\n",
    "    AdaIN1 --> Noise1[\"Noise Injection<br>Learned per-pixel noise\"]\n",
    "    Noise1 --> Act1[\"Activation<br>LeakyReLU\"]\n",
    "    \n",
    "    Act1 --> Upsample1[\"Upsample<br>4×4 → 8×8\"]\n",
    "    Upsample1 --> Conv2[\"Conv Layer 2<br>3×3 convolution\"]\n",
    "    Conv2 --> AdaIN2[\"AdaIN Layer 2<br>Normalization + Style\"]\n",
    "    A2 --> AdaIN2\n",
    "    AdaIN2 --> Noise2[\"Noise Injection<br>Fine detail control\"]\n",
    "    Noise2 --> Act2[\"Activation<br>LeakyReLU\"]\n",
    "    \n",
    "    Act2 --> MoreLayers[\"... more conv blocks<br>Progressive upsampling<br>8×8 → 16×16 → ... → 1024×1024\"]\n",
    "    A3 --> MoreLayers\n",
    "    ADots --> MoreLayers\n",
    "    \n",
    "    MoreLayers --> FinalConv[\"Final Conv Layer<br>Convert to RGB\"]\n",
    "    An --> FinalConv\n",
    "    FinalConv --> Output[\"Generated Image<br>High-resolution RGB<br>1024×1024 or higher\"]\n",
    "    \n",
    "    style Z fill:#BCFB89\n",
    "    style MappingNet fill:#9AE4F5\n",
    "    style W fill:#FBF266\n",
    "    style A1 fill:#FA756A\n",
    "    style A2 fill:#FA756A\n",
    "    style A3 fill:#FA756A\n",
    "    style An fill:#FA756A\n",
    "    style Const fill:#0096D9\n",
    "    style AdaIN1 fill:#FCEB14\n",
    "    style AdaIN2 fill:#FCEB14\n",
    "    style Output fill:#FE9237\n",
    "```\n",
    "\n",
    "##### Understanding Style Control at Different Scales\n",
    "\n",
    "One of StyleGAN's most remarkable features is how different layers control different aspects of the generated image. This hierarchical control emerges naturally from the architecture and provides intuitive editing capabilities.\n",
    "\n",
    "**Coarse Styles (Low-resolution layers, 4×4 to 8×8)**: These layers control high-level aspects such as pose, general hair style, face shape, and eyeglasses. Changes at this level affect the overall composition and major structural elements of the image.\n",
    "\n",
    "**Middle Styles (Medium-resolution layers, 16×16 to 32×32)**: These layers govern finer facial features, hair style details, eyes open or closed, and other mid-level attributes. This is where many recognizable facial characteristics are determined.\n",
    "\n",
    "**Fine Styles (High-resolution layers, 64×64 to 1024×1024)**: These layers control micro-features such as skin texture, hair color variations, background details, and fine-grained lighting effects. Changes here affect the surface appearance without altering the overall structure.\n",
    "\n",
    "This hierarchical organization allows for incredibly precise control. You can modify just the hair color of a generated person without affecting their facial structure, or change their pose without altering their identity—capabilities that were previously impossible with traditional GANs.\n",
    "\n",
    "##### Progressive Growing and Training Stability\n",
    "\n",
    "StyleGAN builds upon the progressive growing technique introduced in Progressive GAN, where the network starts by learning to generate low-resolution images and gradually adds layers to increase resolution. This approach provides several advantages for training stability and final image quality.\n",
    "\n",
    "The progressive training begins with 4×4 images and progressively adds layers to reach the final resolution. During this process, both the generator and discriminator grow in tandem, with new layers being gradually faded in rather than added abruptly. This smooth transition prevents training instabilities that often occur when suddenly changing the network architecture.\n",
    "\n",
    "The mathematical formulation for the fade-in process involves a mixing parameter $\\alpha$ that gradually transitions from the lower-resolution output to the higher-resolution output:\n",
    "$$\\text{output} = (1 - \\alpha) \\cdot \\text{upsampled\\_low\\_res} + \\alpha \\cdot \\text{high\\_res}$$\n",
    "\n",
    "This progressive approach allows the network to first learn the overall structure and composition at low resolution, then refine details at higher resolutions. This mirrors how human artists often work, sketching the overall composition before adding fine details.\n",
    "\n",
    "##### The Power of Style Mixing\n",
    "\n",
    "StyleGAN's architecture enables a fascinating capability called style mixing, where different parts of the style vector $w$ can come from different source images. This creates a powerful tool for controlled image synthesis and editing.\n",
    "\n",
    "In style mixing, you generate two different style vectors $w_1$ and $w_2$ from two random noise vectors $z_1$ and $z_2$. Then, you use $w_1$ for some layers and $w_2$ for others. For example, you might use $w_1$ for coarse layers (controlling face structure) and $w_2$ for fine layers (controlling skin texture and hair details).\n",
    "\n",
    "This technique reveals the disentangled nature of StyleGAN's representation. When you mix styles from two different faces, you get coherent combinations rather than nonsensical blends. The face structure from one person can be seamlessly combined with the hair style from another, creating novel but realistic combinations.\n",
    "\n",
    "##### Truncation Trick and Quality Control\n",
    "\n",
    "StyleGAN introduces a clever technique called the truncation trick to balance between diversity and quality in generated images. The insight is that samples from the extreme regions of the latent space often produce lower-quality images, while samples closer to the center tend to produce higher-quality but less diverse results.\n",
    "\n",
    "The truncation trick modifies the sampling process by constraining the latent vectors:\n",
    "$$w' = \\bar{w} + \\psi(w - \\bar{w})$$\n",
    "\n",
    "where $\\bar{w}$ is the average latent vector computed over many samples, and $\\psi$ is the truncation factor. When $\\psi = 1$, you get the original sampling distribution. When $\\psi < 1$, you get samples closer to the average, typically resulting in higher quality but reduced diversity.\n",
    "\n",
    "This technique provides a dial for controlling the quality-diversity tradeoff, which proves invaluable for practical applications where consistent quality is more important than maximum diversity.\n",
    "\n",
    "##### Perceptual Path Length and Disentanglement\n",
    "\n",
    "StyleGAN introduces a novel metric called Perceptual Path Length (PPL) to measure the quality of the latent space representation. This metric quantifies how smoothly the generated images change as you move through the latent space. A good latent space should produce smooth, meaningful transitions rather than abrupt, nonsensical changes.\n",
    "\n",
    "The PPL metric is computed by measuring the perceptual distance between images generated from nearby points in the latent space:\n",
    "$$\\text{PPL} = \\mathbb{E}\\left[\\frac{1}{\\epsilon^2} d(\\text{G}(w), \\text{G}(w + \\epsilon \\delta))\\right]$$\n",
    "\n",
    "where $d$ is a perceptual distance function (typically using VGG features), $\\epsilon$ is a small step size, and $\\delta$ is a random direction in the latent space.\n",
    "\n",
    "Lower PPL values indicate smoother, more consistent interpolations between generated images, which correlates with better disentanglement and more meaningful latent representations.\n",
    "\n",
    "##### StyleGAN2: Addressing Artifacts and Improving Quality\n",
    "\n",
    "The success of StyleGAN led to StyleGAN2, which addressed several artifacts present in the original architecture. The most notable issues were droplet-like artifacts that appeared in generated images, particularly noticeable in fine details like hair and water.\n",
    "\n",
    "StyleGAN2 introduced several architectural improvements. The AdaIN operation was modified to prevent information from bypassing the style modulation. The progressive growing was replaced with residual connections and improved network designs. Most importantly, the generator architecture was redesigned to eliminate the characteristic artifacts while maintaining the controllability that made StyleGAN famous.\n",
    "\n",
    "The path length regularization was also improved in StyleGAN2, leading to even smoother and more disentangled latent spaces. These improvements resulted in significantly higher image quality and more reliable controllability.\n",
    "\n",
    "##### Applications and Impact\n",
    "\n",
    "StyleGAN has revolutionized numerous applications in computer graphics, entertainment, and research. In the entertainment industry, it enables rapid prototyping of character designs and concept art. Fashion and beauty industries use it for virtual try-ons and style exploration. Researchers employ it to generate synthetic datasets for training other machine learning models.\n",
    "\n",
    "The controllability of StyleGAN has opened new possibilities in image editing. Applications like semantic face editing, where users can adjust specific facial attributes with simple sliders, become possible thanks to the disentangled latent space. This level of control was previously achievable only through manual photo editing by skilled artists.\n",
    "\n",
    "Perhaps most importantly, StyleGAN has demonstrated that generative models can achieve both high quality and meaningful controllability simultaneously. This combination has influenced the design of subsequent generative models across different domains, from text generation to 3D modeling.\n",
    "\n",
    "##### Theoretical Implications and Future Directions\n",
    "\n",
    "StyleGAN's success has important theoretical implications for understanding generative models. It demonstrates that the choice of latent space representation significantly affects the quality and controllability of generated content. The mapping network's role in transforming a simple noise distribution into a more structured intermediate representation provides insights into how neural networks can learn meaningful data representations.\n",
    "\n",
    "The hierarchical control mechanism in StyleGAN suggests that natural images have an inherent hierarchical structure that can be exploited for better generation and editing. This insight has influenced research in other domains, leading to hierarchical generative models for text, audio, and 3D content.\n",
    "\n",
    "Current research directions include extending StyleGAN's principles to other domains, improving the disentanglement of latent representations, and developing more efficient training procedures. The fundamental insights from StyleGAN continue to shape the development of next-generation generative models, promising even more powerful and controllable content creation tools in the future.\n",
    "\n",
    "StyleGAN represents more than just a technical advancement; it embodies a new philosophy of generative modeling where controllability and quality go hand in hand. By understanding the principles behind StyleGAN, we gain insights not only into current capabilities but also into the future possibilities of AI-driven content creation. The architecture's elegant solution to the disentanglement problem continues to inspire new research directions and practical applications, making it a cornerstone of modern generative artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise injection\n",
    "\n",
    "The noise injection helps with something that the authors call **stochastic variation**. They argue that many aspects of a human face are stochastic, such as hair curls or freckles. By adding random noise at different levels in the generator, they can create more variability without changing the overall image. For example in the image below, we can see how different noise vectors impacts the placement of the hair.\n",
    "\n",
    "<br>\n",
    "<img src='../images/stochastic_variation.png' width=40% />\n",
    "<br>\n",
    "\n",
    "After each convolution layer in the generator, the authors added a noise injection layer. A random gaussian noise is added to the output and scaled by a learned factor. Let's look at an example together:\n",
    "* let's say that the output shape of the convolution layer is `(1, 256, 32, 32)` where 256 is the number of channels and 32x32 the spatial dimensions.\n",
    "* we create a random noise matrix of dimensions `(1, 1, 32, 32)`\n",
    "* we multiply the above random by a learned scaling factor vector of dimensions `(1, 256, 1, 1)`. This learned scaling factor is initialized with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tests\n",
    "\n",
    "class ApplyNoise(nn.Module):\n",
    "    \"\"\"StyleGAN noise injection layer that adds scaled Gaussian noise to feature maps.\n",
    "    \n",
    "    This layer implements the noise injection mechanism used in StyleGAN architectures.\n",
    "    It adds per-channel scaled noise to the input tensor, where the scaling factors\n",
    "    are learned parameters. The noise is sampled independently for each spatial location.\n",
    "\n",
    "    Attributes:\n",
    "        channels (int): Number of input/output channels.\n",
    "        weights (nn.Parameter): Learnable scaling factors for noise injection,\n",
    "            shape (1, channels, 1, 1).\n",
    "\n",
    "    Example:\n",
    "        >>> noise_layer = ApplyNoise(channels=512)\n",
    "        >>> x = torch.randn(1, 512, 64, 64)\n",
    "        >>> noisy_x = noise_layer(x)\n",
    "        >>> noisy_x.shape\n",
    "        torch.Size([1, 512, 64, 64])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels: int):\n",
    "        \"\"\"Initialize noise injection layer.\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input feature map channels.\n",
    "        \"\"\"\n",
    "        super(ApplyNoise, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.weights = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply scaled noise to input tensor.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, channels, height, width).\n",
    "            \n",
    "        Returns:\n",
    "            Tensor with same shape as input, with learned noise added.\n",
    "            \n",
    "        Note:\n",
    "            - Noise is resampled for each forward pass (different each call)\n",
    "            - Weights are initialized to zero (no noise initially)\n",
    "        \"\"\"\n",
    "        noise = torch.randn(1, 1, x.shape[2], x.shape[3], device=x.device)\n",
    "        return x + self.weights * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_noise = ApplyNoise(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic noise layer structure verified\n"
     ]
    }
   ],
   "source": [
    "tests.check_apply_noise(apply_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive instance normalization\n",
    "\n",
    "\n",
    "The Adaptive instance normalization (AdaIN) is a variation of the **Instance Normalization layer**. In the course, we have discussed about the importance of Batch Normalizations layers. However, we also have seen that in some cases (eg, when using gradient penalties), Batch Normalization is not the preferred type of normalization layer. \n",
    "\n",
    "This figure from the [Group Normalization](https://arxiv.org/pdf/1803.08494.pdf) paper helps to understand the differences between the normalization layers. In the figure below, $H$ and $W$ are the spatial dimensions, $C$ the channel dimension and $N$ the batch dimension.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src='../images/normalization_layers.png' width=80% />\n",
    "<br>\n",
    "\n",
    "In [**Batch Normalization**](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), we normalize pixels of the same channel, accross the batch and spatial dimensions.\n",
    "\n",
    "In [**Layer Normalization**](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html), we normalize pixels of the same batch index, accross the channel and spatial dimensions.\n",
    "\n",
    "In [**Instance Normalization**](https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html), we normalize pixels of the same batch index and channel, accross the spatial dimensions only.\n",
    "\n",
    "In [**Group Normalization**](https://pytorch.org/docs/stable/generated/torch.nn.GroupNorm.html), we group pixels of the batch index together. \n",
    "\n",
    "The AdaIN layer is an extension of the Instance Normalization layer. It takes as input both the output of the previous convolution layer and the latent vector $w$. Then it performs the following:\n",
    "* map the latent vector $w$ to styles vector $(y_{s}, y_{b})$ through learned affine transformations (fully connected layers).\n",
    "* calculate the output of the layer using the following equation: $y_{s} * In(x) + y_{b}$ where $In(x)$ is the input $x$ fed through an instance normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    \"\"\"Adaptive Instance Normalization layer for StyleGAN architectures.\n",
    "    \n",
    "    Implements feature map normalization conditioned on a latent vector w,\n",
    "    allowing style transfer through learned affine transformations.\n",
    "\n",
    "    Attributes:\n",
    "        channels (int): Number of input/output channels\n",
    "        w_dim (int): Dimension of latent style vector\n",
    "        instance_norm (nn.InstanceNorm2d): Instance normalization layer\n",
    "        linear_s (nn.Linear): Style network for scaling (gamma)\n",
    "        linear_b (nn.Linear): Style network for bias (beta)\n",
    "\n",
    "    Example:\n",
    "        >>> adain = AdaIN(channels=512, w_dim=128)\n",
    "        >>> x = torch.randn(4, 512, 64, 64)  # Input features\n",
    "        >>> w = torch.randn(4, 128)          # Style vector\n",
    "        >>> out = adain(x, w)\n",
    "        >>> out.shape\n",
    "        torch.Size([4, 512, 64, 64])\n",
    "\n",
    "    Note:\n",
    "        - Expects input tensor shapes [N, C, H, W] and [N, W_DIM]\n",
    "        - Output maintains same spatial dimensions as input\n",
    "        - Implements equation: output = (x_norm * gamma) + beta\n",
    "\n",
    "    Adaptive Instance Normalization layer\n",
    "    \n",
    "    args:\n",
    "    - channels: number of channels of the input\n",
    "    - w_dim: dimension of the latent vector w\n",
    "    \n",
    "    inputs:\n",
    "    - x: float32 tensor of dim [N, C, H, W]\n",
    "    - w: float32 tensor of dim [N, W_DIM]    \n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, w_dim: int):\n",
    "        \"\"\"Initialize AdaIN layer.\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input feature channels\n",
    "            w_dim: Dimension of style latent vector\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.w_dim = w_dim\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.linear_s = nn.Linear(w_dim, channels)  # Gamma network\n",
    "        self.linear_b = nn.Linear(w_dim, channels)  # Beta network\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply adaptive instance normalization.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape [N, C, H, W]\n",
    "            w: Style vector of shape [N, W_DIM]\n",
    "            \n",
    "        Returns:\n",
    "            Normalized and styled tensor of same shape as input\n",
    "        \"\"\"\n",
    "        x = self.instance_norm(x)       \n",
    "        ys = self.linear_s(w)[..., None, None]  # Add spatial dims\n",
    "        yb = self.linear_b(w)[..., None, None]\n",
    "        return x * ys + yb  # Scale and shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adain = AdaIN(512, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats, you successfully implemented the AdaIN layer!\n"
     ]
    }
   ],
   "source": [
    "tests.check_adain(adain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
